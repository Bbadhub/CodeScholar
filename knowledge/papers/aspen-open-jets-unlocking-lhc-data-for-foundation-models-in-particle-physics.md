# Aspen Open Jets: unlocking LHC data for foundation models in particle physics

## Access

| Field | Value |
|-------|-------|
| DOI | `10.1088/2632-2153/ade58f` |
| Full Paper | [https://doi.org/10.1088/2632-2153/ade58f](https://doi.org/10.1088/2632-2153/ade58f) |
| Source | [https://journalclub.io/episodes/aspen-open-jets-unlocking-lhc-data-for-foundation-models-in-particle-physics](https://journalclub.io/episodes/aspen-open-jets-unlocking-lhc-data-for-foundation-models-in-particle-physics) |
| Year | 2026 |
| Citations | 0 |
| Authors |  |
| Paper ID | `a822cf06-450e-44f6-b070-3742dc470430` |

## Classification

- **Problem Type:** transfer learning in particle physics
- **Domain:** Machine Learning & AI
- **Sub-domain:** foundation models, particle physics
- **Technique:** OMNIJET-α
- **Technique Category:** neural_architecture
- **Type:** novel

## Summary

The paper presents the ASPENOPENJETS (AOJs) dataset, which consists of 178 million high pT jets derived from CMS 2016 Open Data, and demonstrates how pre-training the OMNIJET-α foundation model on this dataset improves performance in generating boosted top and QCD jets. Engineers should care because this approach leverages transfer learning to enhance model performance on downstream tasks with limited data.

## Key Contribution

**Introduction of the ASPENOPENJETS dataset and demonstration of its utility in pre-training foundation models for particle physics applications.**

## Problem

The need for effective machine learning models to analyze complex data generated by the Large Hadron Collider (LHC).

## Method

**Approach:** The method involves pre-training the OMNIJET-α model on the AOJs dataset to learn general features of jets, followed by fine-tuning on specific datasets like JetClass to adapt to different jet types. This bifurcated strategy allows the model to generalize while also specializing in the nuances of real-world data.

**Algorithm:**

1. 1. Preprocess the AOJs dataset to extract jet features.
2. 2. Tokenize the jet features using a VQ-VAE model.
3. 3. Pre-train the OMNIJET-α model on the tokenized AOJs dataset.
4. 4. Fine-tune the pre-trained model on the JetClass dataset for specific jet generation tasks.
5. 5. Evaluate the model's performance using Kullback–Leibler divergence and Wasserstein-1 distance metrics.

**Input:** Tokenized jet features from the AOJs dataset.

**Output:** Generated jet sequences that can be decoded back into physical space.

**Key Parameters:**

- `codebook_size: 8192`
- `batch_size: 256`
- `learning_rate: not stated`
- `optimizer: Ranger`

**Complexity:** not stated

## Benchmarks

**Tested on:** ASPENOPENJETS (AOJs), JetClass

**Results:**

- Kullback–Leibler divergence
- Wasserstein-1 distance

**Compared against:** Models trained from scratch on JetClass

**Improvement:** Fine-tuned models achieved better performance with fewer training examples compared to models trained from scratch.

## Implementation Guide

**Data Structures:** Tokenized sequences of jet features, Neural network model architecture

**Dependencies:** PyTorch, scikit-learn, scipy

**Core Operation:**

```python
model.train(data) # Pre-train on AOJs dataset; model.fine_tune(data) # Fine-tune on JetClass dataset.
```

**Watch Out For:**

- Ensure proper tokenization of jet features to avoid loss of information.
- Monitor for overfitting during fine-tuning, especially with small datasets.
- Be cautious of domain shifts between pre-training and fine-tuning datasets.

## Use This When

- You have access to large datasets from particle physics experiments.
- You need to generate synthetic data for jet physics.
- You want to leverage transfer learning to improve model performance on small datasets.

## Don't Use When

- The dataset is too small to benefit from pre-training.
- The task requires real-time processing with strict latency constraints.
- You are working with data types not represented in the pre-training dataset.

## Key Concepts

transfer learning, foundation models, jet physics, deep learning, tokenization, generative models

## Connects To

- VQ-VAE
- GPT-style models
- transfer learning techniques
- jet classification algorithms

## Prerequisites

- Understanding of deep learning architectures
- Familiarity with transfer learning concepts
- Knowledge of particle physics data

## Limitations

- Performance may saturate with larger datasets, diminishing returns observed.
- Model may lock into suboptimal parameter space during pre-training.
- Requires significant computational resources for training.

## Open Questions

- What are the optimal tokenization strategies for different jet types?
- How can the model be adapted for real-time analysis of LHC data?

## Abstract

What makes FTL distinctive is how it leverages transfer learning. The encoder is first trained on large volumes of synthetic data generated by high-fidelity simulations, such as those produced by something called the GTC (the Gyrokinetic Toroidal Code). After this initial pretraining phase, the decoder is selectively fine-tuned using a smaller, more targeted set of real or nonlinear simulation data that includes temporally correlated plasma dynamics. This bifurcated strategy enables FTL to learn general spatial features broadly, while still adapting to specific nonlinear behaviors relevant to real-world instability prediction. The system’s design reflects a balance between generalization and specificity. During pretraining, the encoder learns to identify and encode the underlying geometry and spatial mode structures present across a wide range of plasma states. These are treated as "static snapshots", meaning they don’t contain time-sequenced information but represent individual frames of possible plasma configurations.
