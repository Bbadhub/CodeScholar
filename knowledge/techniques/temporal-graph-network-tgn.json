{
  "technique_name": "Temporal Graph Network (TGN)",
  "aliases": [],
  "category": "neural_architecture",
  "one_liner": "TGN is a neural network architecture designed to process dynamic graphs that evolve over time for tasks such as fraud detection.",
  "how_it_works": "Temporal Graph Networks utilize an Event-Based Temporal Graph (ETG) to represent transactions as a dynamic graph where vertices and edges change over time based on transaction events. The TGN employs a message-passing mechanism to compute messages from neighboring vertices, aggregate these messages, and update memory states. This allows the model to learn embeddings for vertices involved in transactions and predict their class labels as fraudulent or legitimate.",
  "algorithm": {
    "steps": [
      "1. Initialize memory states and raw messages.",
      "2. For each epoch, iterate through batches of events in the training set.",
      "3. Compute messages based on the current state.",
      "4. Aggregate messages from neighboring vertices.",
      "5. Update memory states with aggregated messages.",
      "6. Compute embeddings for the vertices involved in the transaction.",
      "7. Apply a sigmoid function to predict the class of the transaction.",
      "8. Calculate the weighted cross-entropy loss and update model parameters."
    ],
    "core_equation": "output = sigmoid(embedding \u00b7 W + b)",
    "input_format": "Event-Based Temporal Graph (ETG) containing vertices, edges, and timestamps of transactions.",
    "output_format": "Predicted class labels for transactions (fraudulent or normal) along with evaluation metrics like F1 score and AUC."
  },
  "parameters": [
    {
      "name": "train_ratio",
      "typical_value": "0.7",
      "effect": "Affects the amount of data used for training."
    },
    {
      "name": "validation_ratio",
      "typical_value": "0.15",
      "effect": "Determines the size of the validation set for tuning."
    },
    {
      "name": "test_ratio",
      "typical_value": "0.15",
      "effect": "Sets aside data for final evaluation."
    },
    {
      "name": "n_epochs",
      "typical_value": "100",
      "effect": "Controls how many times the model will see the training data."
    }
  ],
  "complexity": {
    "time": "Not explicitly stated.",
    "space": "Not explicitly stated.",
    "practical_note": "Performance may vary based on the size and complexity of the input graph."
  },
  "use_when": [
    "You need to detect fraudulent transactions in a dynamic environment.",
    "You have access to time-stamped transaction data.",
    "You want to leverage the relationships between different entities involved in transactions."
  ],
  "avoid_when": [
    "You are working with static transaction data without temporal information.",
    "You require real-time processing with extremely low latency.",
    "You have a very small dataset that cannot effectively train a neural network."
  ],
  "implementation_skeleton": "def temporal_graph_network(data: ETG) -> List[str]:\n    initialize_memory_states()\n    for epoch in range(n_epochs):\n        for batch in get_batches(data):\n            messages = compute_messages(batch)\n            aggregated_messages = aggregate(messages)\n            update_memory_states(aggregated_messages)\n            embeddings = compute_embeddings(batch)\n            predictions = sigmoid(embeddings)\n            update_model_parameters(predictions)\n    return predictions",
  "common_mistakes": [
    "Neglecting to preprocess the temporal data correctly.",
    "Using an inappropriate train-validation-test split.",
    "Failing to tune hyperparameters effectively."
  ],
  "tradeoffs": {
    "strengths": [
      "Effectively captures temporal dependencies in data.",
      "Utilizes relationships between entities for better predictions.",
      "Can improve detection metrics with more interaction events."
    ],
    "weaknesses": [
      "Requires a substantial amount of data for training.",
      "May not perform well with static datasets.",
      "Complexity can lead to longer training times."
    ],
    "compared_to": [
      {
        "technique": "Traditional machine learning algorithms",
        "verdict": "Use TGN for dynamic data; traditional methods may suffice for static data."
      }
    ]
  },
  "connects_to": [
    "Graph Neural Networks (GNN)",
    "Recurrent Neural Networks (RNN)",
    "Temporal Convolutional Networks (TCN)",
    "Dynamic Graph Representation Learning"
  ],
  "maturity": "emerging"
}