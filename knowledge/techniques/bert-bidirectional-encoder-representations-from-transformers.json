{
  "technique_name": "BERT (Bidirectional Encoder Representations from Transformers)",
  "aliases": [
    "BERT"
  ],
  "category": "neural_architecture",
  "one_liner": "BERT is a transformer-based model designed for understanding the context of words in a sentence by considering both left and right context simultaneously.",
  "how_it_works": "BERT uses a transformer architecture that processes text bidirectionally, allowing it to capture context from both sides of a word. It is pre-trained on a large corpus of text and can be fine-tuned for specific tasks such as token classification. In applications like name redaction, BERT can classify tokens as either names or non-names, improving accuracy through fine-tuning on relevant datasets.",
  "algorithm": {
    "steps": [
      "1. Collect and preprocess training data from relevant datasets.",
      "2. Fine-tune the BERT model on the combined training set.",
      "3. Implement a binary classification for each token in the narratives.",
      "4. Use a rule-based classifier to identify common patterns.",
      "5. Evaluate the model on a separate test set.",
      "6. Analyze performance metrics including precision, recall, and false positive rates."
    ],
    "core_equation": "output = softmax(BERT(input))",
    "input_format": "Narrative text from adverse event reports (string format).",
    "output_format": "Redacted text with names replaced by asterisks (string format)."
  },
  "parameters": [
    {
      "name": "learning_rate",
      "typical_value": "1e-5",
      "effect": "A lower learning rate may improve fine-tuning stability but slow down convergence."
    },
    {
      "name": "epochs",
      "typical_value": "5",
      "effect": "More epochs can lead to better performance but may risk overfitting."
    },
    {
      "name": "classification_threshold",
      "typical_value": "0.9",
      "effect": "Increasing the threshold may reduce false positives but could also lower recall."
    }
  ],
  "complexity": {
    "time": "O(n * m), where n is the number of tokens and m is the model size.",
    "space": "O(m), where m is the model size.",
    "practical_note": "While BERT is powerful, it can be resource-intensive, requiring significant memory and processing power."
  },
  "use_when": [
    "You need to redact names in clinical narratives.",
    "You are working with sensitive patient data that requires de-identification.",
    "You want to maintain clinical context while anonymizing reports."
  ],
  "avoid_when": [
    "The dataset has a high prevalence of names that are not well-represented in training data.",
    "Real-time processing is required with very low latency.",
    "You need 100% precision in name redaction."
  ],
  "implementation_skeleton": "def redact_names(narrative: str) -> str:\n    model = load_pretrained_bert()\n    tokens = tokenize(narrative)\n    predictions = model.predict(tokens)\n    redacted = replace_names_with_asterisks(tokens, predictions)\n    return redacted",
  "common_mistakes": [
    "Not fine-tuning the model on domain-specific data.",
    "Using a fixed classification threshold without validation.",
    "Neglecting to preprocess input text properly."
  ],
  "tradeoffs": {
    "strengths": [
      "High accuracy in understanding context due to bidirectional training.",
      "Versatile and can be fine-tuned for various NLP tasks.",
      "Effective in handling complex language patterns."
    ],
    "weaknesses": [
      "Resource-intensive, requiring significant computational power.",
      "May struggle with out-of-distribution names not seen during training.",
      "Long inference times compared to simpler models."
    ],
    "compared_to": [
      {
        "technique": "Rule-based classifiers",
        "verdict": "Use BERT for better context understanding; use rule-based for simpler, faster tasks."
      }
    ]
  },
  "connects_to": [
    "Transformers",
    "Token classification",
    "Natural Language Processing (NLP)",
    "Fine-tuning techniques"
  ],
  "maturity": "proven"
}